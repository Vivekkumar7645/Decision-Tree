{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1. What is a Decision Tree, and how does it work?\n",
        "**Ans** - A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It works by splitting data into branches based on feature conditions, forming a tree-like structure where each internal node represents a decision rule, each branch represents an outcome, and each leaf node represents a final decision or prediction.\n",
        "\n",
        "**Working of Decision Tree**\n",
        "1. Start with the Root Node\n",
        "  * The entire dataset is considered at the root.\n",
        "  * The best feature is selected to split the data based on some criteria.\n",
        "\n",
        "2. Splitting the Data\n",
        "  * The dataset is divided into subsets based on feature values.\n",
        "  * Each subset is assigned to a new node.\n",
        "\n",
        "3. Recursive Splitting\n",
        "  * The process is repeated for each child node until one of the stopping conditions is met:\n",
        "  * All data in a node belongs to the same class.\n",
        "  * A predefined depth limit is reached.\n",
        "  * Further splitting does not improve the model significantly.\n",
        "\n",
        "4. Leaf Nodes\n",
        "  * Once splitting stops, leaf nodes are created, representing the final decision or class label.\n",
        "\n",
        "**Types of Decision Trees**\n",
        "* Classification Tree: Used when the output is categorical.\n",
        "* Regression Tree: Used when the output is continuous.\n",
        "\n",
        "**Splitting Criteria**\n",
        "* Gini Impurity: Measures how often a randomly chosen element would be incorrectly classified.\n",
        "* Entropy & Information Gain: Measures the impurity reduction.\n",
        "* Mean Squared Error: Measures variance reduction.\n",
        "\n",
        "**Advantages**\n",
        "* Easy to understand and interpret.\n",
        "* Handles both numerical and categorical data.\n",
        "* Requires little data preprocessing.\n",
        "\n",
        "**Disadvantages**\n",
        "* Can overfit if too deep.\n",
        "* Sensitive to small variations in data.\n",
        "* Not always the most accurate model compared to other algorithms like Random Forest."
      ],
      "metadata": {
        "id": "VxDnUy2-fkFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. What are impurity measures in Decision Trees?\n",
        "**Ans** - **Impurity Measures in Decision Trees**\n",
        "\n",
        "Impurity measures help determine how well a dataset is split at each node of a Decision Tree. A node is considered \"pure\" if all the data points in it belong to the same class. The goal of the decision tree is to reduce impurity at each step.\n",
        "\n",
        "**1. Gini Impurity**\n",
        "* Measures how often a randomly chosen element would be incorrectly classified if randomly labeled according to the distribution of labels in the node.\n",
        "\n",
        "* Formula:\n",
        "\n",
        "      Gini = 1-∑ᶜᵢ₌₁ pᵢ²\n",
        "where pᵢ is the probability of a class i in the node.\n",
        "\n",
        "* Example:\n",
        "If a node has 70% class A and 30% class B,\n",
        "\n",
        "      Gini = 1-(0.7²+0.3²) = 1−(0.49+0.09) = 0.42\n",
        "A lower Gini value means a purer node.\n",
        "\n",
        "* Used In: CART (Classification and Regression Trees).\n",
        "\n",
        "**2. Entropy (Information Gain)**\n",
        "* Measures the disorder in a dataset.\n",
        "* Formula:\n",
        "\n",
        "      Entropy = -∑ᶜᵢ₌₁pᵢlog₂pᵢ\n",
        "* Example:\n",
        "\n",
        "If a node has 50% class A and 50% class B,\n",
        "\n",
        "    Entropy = -[0.5log₂(0.5)+0.5log₂(0.5)]\n",
        "            = -[0.5(−1)+0.5(−1)] = 1\n",
        "Higher entropy means more impurity.\n",
        "* Used In: ID3, C4.5, and C5.0 decision tree algorithms.\n",
        "\n",
        "**3. Variance Reduction (for Regression Trees)**\n",
        "* Used in regression trees where the output is continuous.\n",
        "* Measures the decrease in variance after splitting.\n",
        "* Formula:\n",
        "\n",
        "      Variance = 1/n∑(yᵢ- ȳ)²\n",
        "  where ȳ is the mean of the target variable.\n",
        "\n",
        "* The split that results in the largest variance reduction is chosen.\n",
        "* Used In: Regression trees.\n",
        "\n",
        "**Comparison of Impurity Measures**\n",
        "\n",
        "|Measure\t|Type\t|Best Value\t|Used In|\n",
        "|-||||\n",
        "|Gini Impurity\t|Classification\t|0 (pure split)\t|CART Algorithm|\n",
        "|Entropy\tClassification\t|0 (pure split)\t|ID3, C4.5|\n",
        "|Variance Reduction\t|Regression\t|Low Variance\t|Regression Trees|"
      ],
      "metadata": {
        "id": "dWrru8Swfkl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. What is the mathematical formula for Gini Impurity?\n",
        "**Ans** - The mathematical formula for Gini Impurity is:\n",
        "\n",
        "    Gini = 1-∑ᶜᵢ₌₁pᵢ²\n",
        "where:\n",
        "  * C = total number of classes\n",
        "  * pᵢ = proportion of class i in the node\n",
        "\n",
        "**Example Calculation**\n",
        "\n",
        "Suppose a node has two classes:\n",
        "* Class A: 70% (pA=0.7p A=0.7)\n",
        "* Class B: 30% (pB=0.3p B=0.3)\n",
        "\n",
        "      Gini = 1-(0.72+0.32)\n",
        "      Gini = 1-(0.49+0.09) = 1-0.58 = 0.42\n",
        "A lower Gini value means the node is purer, meaning the samples in the node mostly belong to one class."
      ],
      "metadata": {
        "id": "V2pdgcj2fk45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. What is the mathematical formula for Entropy?\n",
        "**Ans** - The mathematical formula for Entropy in a Decision Tree is:\n",
        "\n",
        "    Entropy=−∑ᶜᵢ₌₁pᵢlog₂pᵢ\n",
        "\n",
        "where:\n",
        "  * C = total number of classes\n",
        "  * pᵢ = proportion of class i in the node\n",
        "\n",
        "**Example Calculation**\n",
        "\n",
        "Suppose a node has two classes:\n",
        "* Class A: 70% (pA=0.7p A=0.7)\n",
        "* Class B: 30% (pB=0.3p B=0.3)\n",
        "\n",
        "      Entropy=-(0.7log₂0.7+0.3log₂0.3)\n",
        "Using approximate log values:\n",
        "\n",
        "      log₂0.7≈-0.514\n",
        "      log₂0.3≈-1.737\n",
        "      Entropy=-[(0.7×−0.514)+(0.3×−1.737)]\n",
        "            =-[−0.3598−0.5211]\n",
        "            =0.881\n",
        "\n",
        "**Key Points**\n",
        "* Entropy = 0 → Node is pure (all samples belong to one class).\n",
        "* Entropy is highest when classes are equally distributed (e.g., 50%-50% gives Entropy = 1).\n",
        "* Lower entropy means better splits."
      ],
      "metadata": {
        "id": "uLx-x7PyflMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. What is Information Gain, and how is it used in Decision Trees?\n",
        "**Ans** - Information Gain is a measure used in Decision Trees to determine which feature provides the most useful information for classification. It calculates the reduction in entropy after splitting the dataset based on a feature.\n",
        "\n",
        "**Mathematical Formula**\n",
        "\n",
        "    IG = Entropy(Parent)-∑ᵏᵢ₌₁|Sᵢ|/|S|Entropy(Sᵢ)\n",
        "where:\n",
        "* Entropy(Parent) = Entropy before the split\n",
        "*  Sᵢ= Subsets created by splitting the dataset based on a feature\n",
        "* |Sᵢ|/|S| = Weight of each subset (proportion of samples in that subset)\n",
        "* Entropy(Sᵢ) = Entropy of the subset\n",
        "\n",
        "**Information Gain is Used in Decision Trees**\n",
        "1. Calculate Entropy of the Parent Node.\n",
        "2. Split the Data based on a feature.\n",
        "3. Calculate Weighted Entropy of the Child Nodes.\n",
        "4. Compute Information Gain by subtracting the new entropy from the original entropy.\n",
        "5. Choose the Feature with the Highest Information Gain for splitting.\n",
        "\n",
        "**Example Calculation**\n",
        "\n",
        "Suppose we have a dataset with two classes (Yes/No):\n",
        "* Before Splitting (Parent Entropy):\n",
        "  * 5 Yes, 5 No → Entropy = 1\n",
        "* After Splitting on Feature X:\n",
        "  * Left Node: 4 Yes, 1 No → Entropy = 0.72\n",
        "  * Right Node: 1 Yes, 4 No → Entropy = 0.72\n",
        "\n",
        "If each node contains half of the samples:\n",
        "\n",
        "    Entropy(Children) = 5/10(0.72)+5/10(0.72) = 0.72\n",
        "    InformationGain = 1−0.72 = 0.28\n",
        "Since higher Information Gain is better, the feature with the highest IG is chosen for the next split.\n",
        "\n",
        "**Key Points**\n",
        "* Higher Information Gain → More informative feature.\n",
        "* Used in ID3, C4.5, and C5.0 Decision Trees.\n",
        "* Prevents unnecessary splits, improving model efficiency."
      ],
      "metadata": {
        "id": "CiuIq-Qkfldh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. What is the difference between Gini Impurity and Entropy?\n",
        "**Ans** - **Difference Between Gini Impurity and Entropy in Decision Trees**\n",
        "\n",
        "|Criterion\t|Gini Impurity\t|Entropy (Information Gain)|\n",
        "|-|||\n",
        "|Definition\t|Measures the probability of incorrectly classifying a randomly chosen element.\t|Measures the disorder (uncertainty) in a dataset.|\n",
        "|Formula\t|Gini=1-∑pᵢ² |\tEntropy=-∑pᵢlog₂pᵢ|\n",
        "|Range\t|0 (pure node) to 0.50.5 (two equal classes)\t|0 (pure node) to 11 (max disorder for two classes)|\n",
        "|Computation\t|Faster, as it avoids logarithms.\t|Slightly slower due to log calculations.|\n",
        "|Interpretation\t|Lower Gini means purer splits.|\tLower Entropy means less disorder.|\n",
        "|Decision Tree Usage\t|Used in CART (Classification and Regression Trees).\t|Used in ID3, C4.5, C5.0 algorithms.|\n",
        "|Preference\t|Works well in most cases, often default in libraries like Scikit-Learn.\t|Preferred when probability distributions need more precise differentiation.|\n",
        "\n",
        "**Example Comparison**\n",
        "\n",
        "Suppose a node has two classes:\n",
        "* Class A: 70% (pA=0.7p A =0.7)\n",
        "* Class B: 30% (pB=0.3p B =0.3)\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "        Gini=1-(0.7²+0.3²) = 1-(0.49+0.09) = 0.42\n",
        "2. Entropy\n",
        "\n",
        "        Entropy=-[0.7log₂(0.7)+0.3log₂(0.3)]\n",
        "Approximating logs:\n",
        "\n",
        "        =-[0.7(−0.514)+0.3(−1.737)]\n",
        "        =-(-0.3598-0.5211) = 0.881"
      ],
      "metadata": {
        "id": "GIeoJBhiflvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. What is the mathematical explanation behind Decision Trees?\n",
        "**Ans** - **Mathematical Explanation of Decision Trees**\n",
        "A Decision Tree is a recursive, tree-like structure used for classification and regression. The mathematical foundation relies on selecting the best feature to split the dataset, minimizing impurity and maximizing information gain.\n",
        "\n",
        "**1. Splitting Criteria**\n",
        "\n",
        "At each node, the algorithm evaluates all features and chooses the one that best separates the data. The selection is based on minimizing impurity.\n",
        "\n",
        "**For Classification:**\n",
        "\n",
        "(a) Gini Impurity\n",
        "\n",
        "    Gini = 1−∑ᶜᵢ₌₁pᵢ²\n",
        "where:\n",
        "* C is the number of classes.\n",
        "* pᵢ  is the proportion of class i in the node.\n",
        "\n",
        "(b) Entropy (Information Gain)\n",
        "\n",
        "    Entropy = −∑ᶜᵢ₌₁pᵢlog₂pᵢ\n",
        "Information Gain = Entropy(Parent)-∑ᵏᵢ₌₁|Sᵢ|/|S|Entropy(Sᵢ)\n",
        "where:\n",
        "* S is the total dataset.\n",
        "* Sᵢ are the subsets created by splitting.\n",
        "* |Sᵢ|/|S| is the weight of each subset.\n",
        "\n",
        "**For Regression:**\n",
        "\n",
        "(c) Variance Reduction\n",
        "\n",
        "Used when predicting continuous values.\n",
        "\n",
        "    Variance = 1/n∑(yᵢ-ȳ)²\n",
        "Variance Reduction = Variance(Parent)-∑ᵏᵢ₌₁ |Sᵢ|/|S|Variance(Sᵢ)\n",
        "\n",
        "**2. Recursive Splitting**\n",
        "\n",
        "Once the best feature is chosen, the dataset is split into child nodes. The process repeats until a stopping condition is met:\n",
        "* All data in a node belong to the same class (pure node).\n",
        "* Max depth is reached.\n",
        "* Further splitting does not improve accuracy.\n",
        "\n",
        "**3. Pruning (Avoiding Overfitting)**\n",
        "\n",
        "Pruning helps prevent overfitting by removing unnecessary branches.\n",
        "* Pre-pruning: Stop splitting if Information Gain is below a threshold.\n",
        "* Post-pruning: Build the full tree, then remove weak branches using cross-validation.\n",
        "\n",
        "**Final Prediction**\n",
        "* Classification: The majority class in a leaf node is the predicted label.\n",
        "* Regression: The average value of the target variable in a leaf node is the prediction.\n",
        "\n",
        "**Key Takeaways**\n",
        "* Mathematical foundation: Based on entropy, Gini impurity, or variance reduction.\n",
        "* Recursive splitting: Selects the best feature at each step.\n",
        "* Pruning: Prevents overfitting.\n",
        "* Prediction: Uses majority voting (classification) or averaging (regression)."
      ],
      "metadata": {
        "id": "Pig-eJ5Gfl_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. What is Pre-Pruning in Decision Trees?\n",
        "**Ans** - **Pre-Pruning in Decision Trees**\n",
        "\n",
        "Pre-pruning is a technique used to prevent a Decision Tree from growing too deep, reducing overfitting by stopping the tree from splitting further based on certain conditions.\n",
        "\n",
        "**How Pre-Pruning Works**\n",
        "\n",
        "Instead of allowing the tree to grow fully and then cutting branches, pre-pruning stops the tree early if:\n",
        "1. Max Depth is Reached: Stop splitting when the tree reaches a certain depth.\n",
        "2. Minimum Samples per Leaf: Stop if a node has fewer than a threshold number of samples.\n",
        "3. Minimum Information Gain: Stop if splitting does not significantly reduce impurity (Gini/Entropy).\n",
        "4. Max Number of Nodes: Limit the total number of nodes in the tree.\n",
        "5. Chi-Square Testing (for categorical data): Stop splitting if the new branches do not significantly improve classification.\n",
        "\n",
        "**Mathematical Explanation**\n",
        "\n",
        "A Decision Tree recursively splits nodes by selecting the best feature based on impurity reduction:\n",
        "\n",
        "    Information Gain = Entropy(Parent)-∑ᵏᵢ₌₁|Sᵢ|/|S|Entropy(Sᵢ)\n",
        "Pre-pruning sets a threshold θ, where further splitting occurs only if:\n",
        "\n",
        "    Information Gain>θ\n",
        "Similarly, in Gini Impurity:\n",
        "\n",
        "    Gini Reduction = Gini(Parent)-∑ᵏᵢ₌₁|Sᵢ|/|S|Gini(Sᵢ)\n",
        "Pre-pruning restricts splitting when Gini Reduction is too small.\n",
        "\n",
        "**Advantages of Pre-Pruning**\n",
        "  * Prevents Overfitting: Stops the tree before it becomes too complex.\n",
        "  * Improves Generalization: Produces a simpler model that works better on new data.\n",
        "  * Reduces Training Time: Stops unnecessary computations.\n",
        "\n",
        "**Disadvantages of Pre-Pruning**\n",
        "  * Risk of Underfitting: The tree might be too simple and miss important patterns.\n",
        "  * Choosing the Right Threshold is Hard: Improper stopping criteria can hurt accuracy.\n",
        "\n",
        "**Example in Python (Pre-Pruning using Scikit-Learn)**"
      ],
      "metadata": {
        "id": "W4__BTV4fmSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=3, min_samples_split=5, min_samples_leaf=3)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "LfQIZsyALEhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Takeaways**\n",
        "* Pre-Pruning stops tree growth early based on predefined conditions.\n",
        "* Helps reduce overfitting but may cause underfitting if too aggressive.\n",
        "* Common parameters: max_depth, min_samples_split, min_samples_leaf."
      ],
      "metadata": {
        "id": "8cyNngbNLH-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. What is Post-Pruning in Decision Trees?\n",
        "**Ans** - **Post-Pruning in Decision Trees**\n",
        "\n",
        "Post-pruning (also called \"pruning\" or \"cost-complexity pruning\") is a technique used to reduce overfitting in Decision Trees by removing unnecessary branches after the tree has been fully grown.\n",
        "\n",
        "**How Post-Pruning Works**\n",
        "1. Grow the Full Tree: The Decision Tree is allowed to split until it perfectly classifies all training data.\n",
        "2. Evaluate Performance: A validation dataset is used to measure the accuracy of different subtrees.\n",
        "3. Remove Unnecessary Branches: The least important branches are pruned.\n",
        "4. Stop When Accuracy Stops Improving: The tree is pruned until removing further nodes reduces accuracy.\n",
        "\n",
        "**Mathematical Explanation**\n",
        "\n",
        "A tree's complexity can be controlled by defining a pruning cost function:\n",
        "\n",
        "    C(T)=E(T)+α⋅|T|\n",
        "where:\n",
        "* C(T) = Cost of the tree.\n",
        "* E(T) = Error of the tree (misclassification rate).\n",
        "* |T| = Number of terminal nodes (leaf nodes).\n",
        "* α = Complexity penalty (higher αα leads to more pruning).\n",
        "\n",
        "The goal is to find the subtree T' that minimizes C(T).\n",
        "\n",
        "**Types of Post-Pruning**\n",
        "\n",
        "**1. Reduced Error Pruning**\n",
        "* Remove nodes if accuracy on validation data does not decrease.\n",
        "* Stop when further pruning hurts accuracy.\n",
        "\n",
        "**2. Cost-Complexity Pruning**\n",
        "* Uses an α parameter to control pruning.\n",
        "* A series of subtrees is created, and the one with the best validation accuracy is chosen.\n",
        "\n",
        "**Advantages of Post-Pruning**\n",
        "* Better Generalization: Removes unnecessary complexity while keeping important structure.\n",
        "* Higher Accuracy on Test Data: Avoids overfitting to training data.\n",
        "* Data-Driven Pruning: Uses a validation set, making it more reliable.\n",
        "\n",
        "**Disadvantages of Post-Pruning**\n",
        "* Computationally Expensive: Requires training a fully grown tree first.\n",
        "* Choosing the Right Pruning Parameter: Finding the best α in cost-complexity pruning can be difficult.\n",
        "\n",
        "**Example in Python (Using Cost-Complexity Pruning)**"
      ],
      "metadata": {
        "id": "L7veSk9mfmlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "pruned_trees = [DecisionTreeClassifier(random_state=42, ccp_alpha=alpha).fit(X_train, y_train) for alpha in ccp_alphas]\n",
        "\n",
        "train_scores = [tree.score(X_train, y_train) for tree in pruned_trees]\n",
        "test_scores = [tree.score(X_test, y_test) for tree in pruned_trees]\n",
        "\n",
        "best_alpha = ccp_alphas[test_scores.index(max(test_scores))]\n",
        "print(f\"Best pruning parameter (alpha): {best_alpha}\")"
      ],
      "metadata": {
        "id": "EOhs6dnjMcnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Takeaways**\n",
        "* Post-Pruning removes overfitting after training.\n",
        "* Uses cost-complexity pruning to balance accuracy and complexity.\n",
        "* More effective than pre-pruning but computationally expensive."
      ],
      "metadata": {
        "id": "3ZU2lu1DMgDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. What is the difference between Pre-Pruning and Post-Pruning?\n",
        "**Ans** - **Difference Between Pre-Pruning and Post-Pruning in Decision Trees**\n",
        "\n",
        "|Criterion\t|Pre-Pruning (Early Stopping)\t|Post-Pruning (Pruning After Training)|\n",
        "|-|||\n",
        "|Definition\t|Stops tree growth before it overfits.|\tRemoves unnecessary branches after the tree is fully grown.|\n",
        "|How It Works\t|Prevents splitting if a stopping condition (e.g., max depth, min samples) is met.|\tTrims back branches that don’t improve test accuracy.|\n",
        "|Stopping Criteria\t|- Max tree depth reached.\n",
        "- Min samples per leaf/node.\n",
        "- Minimum information gain.\n",
        "- Chi-square test (for categorical data).|\t- Reduced error pruning (removes branches that don’t improve validation accuracy).\n",
        "- Cost-complexity pruning (uses α to balance complexity & error).|\n",
        "|Goal\t|Prevent unnecessary complexity from the beginning.|\tRemove overfitting caused by excessive depth.|\n",
        "|Computation Cost\t|Faster, as it stops growing early.|\tSlower, since the full tree is grown before pruning.|\n",
        "|Risk\t|May cause underfitting if stopped too early.|\tMore effective in preventing overfitting but computationally expensive.|\n",
        "|Commonly Used In\t|Simple decision trees, small datasets.\t|Large datasets, complex trees.|\n",
        "|Scikit-Learn Parameters\t|max_depth, min_samples_split, min_samples_leaf\t|ccp_alpha (cost-complexity pruning)|\n",
        "\n",
        "**Example Comparison**\n",
        "\n",
        "Imagine a Decision Tree built for classifying emails as spam or not spam:\n",
        "* Pre-Pruning: The tree stops splitting when max_depth = 3, even if some deeper patterns exist.\n",
        "* Post-Pruning: The tree grows fully, then removes branches that don't improve validation accuracy."
      ],
      "metadata": {
        "id": "NztvqreJfm7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. What is a Decision Tree Regressor?\n",
        "**Ans** - **Decision Tree Regressor**\n",
        "\n",
        "A Decision Tree Regressor is a machine learning model that uses a tree structure to predict continuous numerical values rather than categorical labels (as in classification). It splits data recursively based on feature values to minimize prediction error (e.g., variance reduction or mean squared error).\n",
        "\n",
        "**Working**\n",
        "1. Start with the entire dataset as the root node.\n",
        "2. Choose the best split using a criterion like Mean Squared Error, Mean Absolute Error, or Reduction in Variance.\n",
        "3. Split the dataset into child nodes based on the chosen feature and threshold.\n",
        "4. Repeat the process recursively until a stopping condition is met (e.g., max depth, min samples per leaf).\n",
        "5. Predict the output at leaf nodes using the mean of the target values in that node.\n",
        "\n",
        "**Mathematical Formulation**\n",
        "\n",
        "**1. Splitting Criterion**\n",
        "\n",
        "(a) Mean Squared Error\n",
        "\n",
        "To split a node at feature XjX j, select the threshold tt that minimizes:\n",
        "\n",
        "    MSE = 1/N∑ᴺᵢ₌₁(yᵢ-ŷ)²\n",
        " where:\n",
        "* yᵢ is the actual target value.\n",
        "* ŷ is the mean prediction in the node.\n",
        "\n",
        "The goal is to find the split that reduces MSE the most.\n",
        "\n",
        "(b) Variance Reduction\n",
        "\n",
        "Alternatively, the variance before and after splitting is computed:\n",
        "\n",
        "    Variance = 1/N∑ᴺᵢ₌₁(yᵢ-ȳ)²\n",
        "\n",
        "A split is chosen if it maximizes variance reduction:\n",
        "\n",
        "    Variance Reduction = Variance(Parent)-∑ᵏᵢ₌₁|Sᵢ|/|S|Variance(Sᵢ)\n",
        "where\n",
        "* Sᵢ are the subsets created by the split.\n",
        "\n",
        "**Advantages of Decision Tree Regressors**\n",
        "* Easy to Interpret: Can be visualized like a flowchart.\n",
        "* Handles Non-Linear Data: Can model complex relationships.\n",
        "* No Need for Feature Scaling: Works well with raw data.\n",
        "* Handles Missing Values: Can split on available features.\n",
        "\n",
        "**Disadvantages**\n",
        "* Prone to Overfitting: Without pruning, it can create very deep trees.\n",
        "* Not Smooth Predictions: Predictions are constant within regions, leading to step-like outputs.\n",
        "* Sensitive to Small Changes: Small changes in data can result in different trees.\n",
        "\n",
        "**Example: Decision Tree Regression in Python**"
      ],
      "metadata": {
        "id": "131gSo4LfnMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "regressor = DecisionTreeRegressor(max_depth=4)\n",
        "regressor.fit(X, y)\n",
        "\n",
        "X_test = np.linspace(0, 10, 1000).reshape(-1, 1)\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "plt.scatter(X, y, label=\"Actual Data\", color=\"blue\", alpha=0.5)\n",
        "plt.plot(X_test, y_pred, label=\"Decision Tree Prediction\", color=\"red\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GJ6ioPENPNXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. What are the advantages and disadvantages of Decision Trees?\n",
        "**Ans** - **Advantages and Disadvantages of Decision Trees**\n",
        "\n",
        "Decision Trees are widely used in machine learning due to their interpretability and flexibility, but they also have some limitations. Let's go through both aspects.\n",
        "\n",
        "**Advantages of Decision Trees**\n",
        "1. Easy to Understand and Interpret\n",
        "  * Decision Trees are intuitive and can be easily visualized, making them useful for explaining models to non-technical stakeholders.\n",
        "  * They mimic human decision-making processes.\n",
        "\n",
        "2. Handles Both Classification and Regression\n",
        "  * Can be used for both classification (DecisionTreeClassifier) and regression (DecisionTreeRegressor) problems.\n",
        "\n",
        "3. No Need for Feature Scaling\n",
        "  * Unlike SVMs or Neural Networks, Decision Trees don’t require standardization or normalization (e.g., MinMax scaling).\n",
        "\n",
        "4. Handles Non-Linear Relationships\n",
        "  * Can model complex, non-linear decision boundaries without requiring transformations of the data.\n",
        "\n",
        "5. Handles Categorical and Numerical Data\n",
        "  * Works with both categorical and numerical input variables.\n",
        "\n",
        "6. Works with Missing Values\n",
        "  * Can handle missing values by splitting only on available features.\n",
        "\n",
        "7. Feature Selection is Built-In\n",
        "  * Automatically selects the most important features when making splits, reducing the need for manual feature selection.\n",
        "\n",
        "8. Computationally Efficient for Small Datasets\n",
        "  * Faster training compared to deep learning models, especially on smaller datasets.\n",
        "\n",
        "**Disadvantages of Decision Trees**\n",
        "1. Prone to Overfitting\n",
        "  * Deep trees memorize training data, making them perform poorly on new, unseen data.\n",
        "  * Solution: Use pruning (pre-pruning or post-pruning) to prevent overfitting.\n",
        "\n",
        "2. Unstable (Sensitive to Small Changes in Data)\n",
        "  * A small change in the dataset can result in a completely different tree structure.\n",
        "  * Solution: Use an ensemble method like Random Forest to improve stability.\n",
        "\n",
        "3. Greedy Algorithm May Not Find the Optimal Tree\n",
        "  * The tree-building algorithm chooses the best split locally, but not necessarily the best global structure.\n",
        "  * Solution: Use bagging or boosting (e.g., Gradient Boosting, XGBoost) to improve performance.\n",
        "\n",
        "4. Can Be Computationally Expensive for Large Datasets\n",
        "  * As the dataset grows, trees become deep and complex, increasing memory and computation time.\n",
        "  * Solution: Limit tree depth using max_depth or min_samples_split.\n",
        "\n",
        "5. Biased Towards Dominant Classes\n",
        "  * If some classes are more frequent, the tree may favor those classes during splitting.\n",
        "  * Solution: Use balanced class weights or resample the dataset.\n",
        "\n",
        "6. Not Smooth for Regression (Step-Wise Predictions)\n",
        "  * Decision Tree Regression produces piecewise constant predictions, unlike linear or polynomial regression models.\n",
        "  * Solution: Use Random Forest Regressor or Gradient Boosting Regressor for smoother predictions.\n",
        "\n",
        "**Summary Table**\n",
        "\n",
        "|Feature\t|Decision Trees|\n",
        "|-||\n",
        "|Interpretability\t| Highly interpretable|\n",
        "|Feature Scaling Needed?\t| No|\n",
        "|Handles Missing Data?\t| Yes|\n",
        "|Handles Non-Linear Data?\t| Yes|\n",
        "|Overfitting Risk?\t| High (needs pruning)|\n",
        "|Computationally Expensive for Large Datasets?\t| Yes|\n",
        "|Stable Against Small Changes?\t| No (high variance)|\n",
        "|Handles Both Classification & Regression?\t| Yes|"
      ],
      "metadata": {
        "id": "9FUcqd38fneh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. How does a Decision Tree handle missing values?\n",
        "**Ans** -Decision Trees can handle missing values in two main ways:\n",
        "\n",
        "1. During Training (Handling Missing Values While Building the Tree)\n",
        "2. During Prediction (Handling Missing Values When Making Predictions)\n",
        "\n",
        "**1. Handling Missing Values During Training**\n",
        "\n",
        "When training a Decision Tree, missing values in features can be dealt with using different strategies:\n",
        "\n",
        "**(A) Ignore Missing Values When Choosing Splits**\n",
        "* Some Decision Tree algorithms (like CART in Scikit-Learn) ignore samples with missing values when computing the best split.\n",
        "* However, this may reduce the available data and lead to a suboptimal split.\n",
        "\n",
        "(B) Assign Missing Values to the Most Common Value\n",
        "* If a feature is numerical, missing values can be replaced with the mean or median of the feature.\n",
        "* If a feature is categorical, missing values can be replaced with the most frequent category (mode).\n",
        "* This is a simple approach but may introduce bias.\n",
        "\n",
        "(C) Use Surrogate Splits\n",
        "* Instead of removing missing values, some algorithms use alternative (surrogate) features that correlate with the missing feature to determine splits.\n",
        "* If the primary split feature has missing values, the decision tree follows a secondary feature that provides similar information.\n",
        "* Used in C4.5 and CART implementations in some libraries.\n",
        "\n",
        "**2. Handling Missing Values During Prediction**\n",
        "\n",
        "When making predictions, if a sample has a missing value for a split feature, Decision Trees can:\n",
        "\n",
        "(A) Follow the Most Frequent Path\n",
        "* The tree follows the path taken by the majority of samples that have a value at that split.\n",
        "* This is a simple and fast method.\n",
        "\n",
        "(B) Weighted Splitting\n",
        "* Assigns probabilities to missing values based on the distribution of existing values.\n",
        "* The sample is sent down multiple branches proportionally and the prediction is averaged.\n",
        "* More accurate but computationally expensive.\n",
        "\n",
        "**Example: Handling Missing Values in Scikit-Learn**"
      ],
      "metadata": {
        "id": "wGhVCfh6fnvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Feature1': [1, 2, np.nan, 4, 5, 6, np.nan, 8, 9, 10],\n",
        "    'Feature2': [5, np.nan, 2, 3, 4, np.nan, 6, 7, 8, 9],\n",
        "    'Target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
        "})\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data[['Feature1', 'Feature2']] = imputer.fit_transform(data[['Feature1', 'Feature2']])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[['Feature1', 'Feature2']], data['Target'], test_size=0.2, random_state=42)\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Predictions:\", y_pred)"
      ],
      "metadata": {
        "id": "nBhk1cbk-x0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here, we used mean imputation before training the Decision Tree.\n",
        "* Alternatively, Decision Trees like C4.5 can handle missing values internally using surrogate splits."
      ],
      "metadata": {
        "id": "hQhtP_VB-38F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. How does a Decision Tree handle categorical features?\n",
        "**Ans** - **How Decision Trees Handle Categorical Features**\n",
        "Decision Trees can naturally handle categorical features in different ways, depending on whether they use binary splits (CART) or multiway splits (C4.5, ID3).\n",
        "\n",
        "**1. Methods for Handling Categorical Features**\n",
        "\n",
        "Decision Trees handle categorical data in the following ways:\n",
        "\n",
        "(A) One-Hot Encoding\n",
        "* Convert categorical features into multiple binary (0/1) features.\n",
        "* Each category becomes a separate feature column.\n",
        "* Used in Scikit-Learn's DecisionTreeClassifier (CART algorithm).\n",
        "\n",
        "**Advantage:** Works well with all Decision Tree implementations.\n",
        "**Disadvantage:** Increases feature dimensionality if categories are many (high cardinality).\n",
        "\n",
        "* Example:\n",
        "\n",
        "|Color\t|One-Hot Encoding|\n",
        "|-||\n",
        "|Red\t|(1,0,0)|\n",
        "|Blue\t|(0,1,0)|\n",
        "|Green\t|(0,0,1)|"
      ],
      "metadata": {
        "id": "hloP_XEkfoAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform(data[['Color']]).toarray()\n",
        "print(encoded_data)"
      ],
      "metadata": {
        "id": "6cnLGwoN_p0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(B) Label Encoding\n",
        "* Each category is assigned a unique numerical value.\n",
        "* Can be used directly in Decision Trees.\n",
        "* Works well when categories have an inherent order (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "**Advantage:** Efficient, requires fewer features.\n",
        "**Disadvantage:** May introduce false relationships if the order is meaningless.\n",
        "* Example:\n",
        "\n",
        "|Color\t|Label Encoding|\n",
        "|-||\n",
        "|Red\t|0|\n",
        "|Blue\t|1|\n",
        "|Green\t|2|"
      ],
      "metadata": {
        "id": "qgKu5fsEASKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']})\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data['Color'])\n",
        "print(encoded_data)"
      ],
      "metadata": {
        "id": "bPFwe8-4AXH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(C) Decision Trees with Native Categorical Splitting (C4.5, ID3)\n",
        "* Algorithms like C4.5 and ID3 can handle categorical data directly.\n",
        "* They select the best category split based on Information Gain or Gini Impurity.\n",
        "\n",
        "**Advantage:** No need for encoding; works directly with categorical data.\n",
        "**Disadvantage:** Not supported by Scikit-Learn's DecisionTreeClassifier (CART).\n",
        "\n",
        "* Example: If \"Color\" is the best feature to split on, the tree creates branches:"
      ],
      "metadata": {
        "id": "_3drJiwQAdsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "       Color?\n",
        "      /   |   \\\n",
        "   Red   Blue  Green"
      ],
      "metadata": {
        "id": "NBdG3tJEArXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Choosing the Best Approach**\n",
        "\n",
        "|Method\t|Best When\t|Supported in Scikit-Learn?|\n",
        "|-|||\n",
        "|One-Hot Encoding\t|Few categories, no order\t| Yes (works well)|\n",
        "|Label Encoding\t|Categories have natural order\t| Yes (but may mislead)|\n",
        "|Native Categorical Splitting (C4.5)\t|Large categorical features\t| No (Scikit-Learn uses CART)|"
      ],
      "metadata": {
        "id": "cs1aVd1LAut6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. What are some real-world applications of Decision Trees?\n",
        "**Ans** - **Real-World Applications of Decision Trees**\n",
        "\n",
        "Decision Trees are widely used in various domains due to their simplicity, interpretability, and ability to handle mixed data types (numerical & categorical). Here are some key applications:\n",
        "\n",
        "**1. Healthcare & Medical Diagnosis**\n",
        "* Disease Diagnosis – Decision Trees help in diagnosing diseases based on symptoms (e.g., COVID-19, diabetes, cancer detection).\n",
        "* Treatment Recommendation – Used to suggest treatments based on patient history and test results.\n",
        "\n",
        "* Example:\n",
        "\n",
        "A Decision Tree can classify patients as low-risk or high-risk based on age, blood pressure, and cholesterol levels.\n",
        "\n",
        "**2. Finance & Banking**\n",
        "* Credit Scoring & Loan Approval – Used by banks to assess whether a customer is eligible for a loan.\n",
        "* Fraud Detection – Helps identify fraudulent transactions based on unusual spending behavior.\n",
        "\n",
        "* Example:\n",
        "A bank uses a Decision Tree to determine whether to approve a loan based on income, credit history, and debt-to-income ratio.\n",
        "\n",
        "**3. Retail & E-Commerce**\n",
        "* Customer Segmentation – Used to group customers based on purchasing behavior.\n",
        "* Product Recommendation – Helps suggest relevant products to customers.\n",
        "* Churn Prediction – Identifies customers likely to stop using a service.\n",
        "\n",
        "* Example:\n",
        "An e-commerce site uses a Decision Tree to predict if a customer will buy a product based on browsing history, cart abandonment, and past purchases.\n",
        "\n",
        "**4. Manufacturing & Quality Control**\n",
        "* Defect Detection – Helps in identifying defective products based on sensor readings and quality checks.\n",
        "* Predictive Maintenance – Predicts when a machine is likely to fail to prevent breakdowns.\n",
        "\n",
        "* Example:\n",
        "A factory uses a Decision Tree to predict machine failures based on temperature, vibration, and pressure levels.\n",
        "\n",
        "**5.Human Resources & Employee Performance**\n",
        "* Employee Attrition Prediction – Identifies employees who may leave based on work satisfaction, salary, and workload.\n",
        "* Hiring Decisions – Helps in recruitment by analyzing candidate skills, experience, and cultural fit.\n",
        "\n",
        "* Example:\n",
        "A company uses a Decision Tree to determine whether to promote an employee based on performance scores, years of experience, and training completion.\n",
        "\n",
        "**6. Marketing & Advertising**\n",
        "* Targeted Advertising – Determines the best audience for a marketing campaign.\n",
        "* Lead Scoring – Identifies potential customers who are most likely to convert.\n",
        "\n",
        "* Example:\n",
        "An advertising company uses a Decision Tree to decide whether to show an ad based on user age, location, and browsing behavior.\n",
        "\n",
        "**7. Criminal Justice & Law Enforcement**\n",
        "* Crime Prediction – Predicts high-crime areas based on historical data.\n",
        "* Legal Decision Support – Helps lawyers analyze case outcomes based on past rulings.\n",
        "\n",
        "* Example:\n",
        "A law firm uses a Decision Tree to predict the outcome of a case based on evidence, past verdicts, and witness testimonies.\n",
        "\n",
        "**8. Energy & Utilities**\n",
        "* Energy Consumption Forecasting – Predicts electricity usage to optimize supply.\n",
        "* Smart Grid Optimization – Helps manage power distribution efficiently.\n",
        "\n",
        "* Example:\n",
        "An energy company uses a Decision Tree to predict peak electricity demand based on weather conditions and historical usage patterns.\n",
        "\n",
        "**9. Education & Student Performance**\n",
        "* Student Performance Prediction – Helps in identifying students who need additional support.\n",
        "* Personalized Learning Paths – Suggests customized learning plans based on student progress.\n",
        "\n",
        "* Example:\n",
        "A school uses a Decision Tree to predict whether a student will pass based on attendance, homework completion, and past grades."
      ],
      "metadata": {
        "id": "V_5bWTrHf8Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "LCurXvmrf84I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.\n",
        "**Ans** - Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy."
      ],
      "metadata": {
        "id": "3OuymE1efoRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy: {accuracy:.2f}\")\n",
        "plt.figure(figsize=(10,6))\n",
        "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B_3psct5ywNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset (preloaded in sklearn).\n",
        "* Splits the dataset into training (80%) and testing (20%) sets.\n",
        "* Uses Gini Impurity as the criterion and max_depth=3 to prevent overfitting.\n",
        "* Trains a Decision Tree Classifier and makes predictions.\n",
        "* Computes and prints the accuracy of the model."
      ],
      "metadata": {
        "id": "h7Nb4t1gy0q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n",
        "**Ans** - Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances."
      ],
      "metadata": {
        "id": "klm5l12FfouY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "feature_importances = clf.feature_importances_\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lpsf9Z8Ozaiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits the data into 80% training and 20% testing.\n",
        "* Trains a Decision Tree Classifier using Gini Impurity.\n",
        "* Prints feature importances, showing which features contribute the most to classification.\n",
        "\n",
        "**Example Output (Feature Importances May Vary):**"
      ],
      "metadata": {
        "id": "LFp9cevBzfVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sepal length (cm): 0.0123\n",
        "sepal width (cm): 0.0276\n",
        "petal length (cm): 0.9213\n",
        "petal width (cm): 0.0388"
      ],
      "metadata": {
        "id": "C2q8IWFNzwAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Higher values mean the feature is more important!"
      ],
      "metadata": {
        "id": "9GuFrHC0zzGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.\n",
        "**Ans** - Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy."
      ],
      "metadata": {
        "id": "L-qJ4EyCfpAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy (Entropy): {accuracy:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lxKo9zK034ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset (built-in in sklearn).\n",
        "* Splits the dataset into training (80%) and testing (20%) sets.\n",
        "* Uses Entropy as the criterion to split nodes.\n",
        "* Trains a Decision Tree Classifier and makes predictions.\n",
        "* Computes and prints the accuracy of the model."
      ],
      "metadata": {
        "id": "JNjLJ8Vo377A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE).\n",
        "**Ans** - Python program to train a Decision Tree Regressor on a housing dataset and evaluate its performance using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "p8RrBn4KfpUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(criterion=\"squared_error\", max_depth=5, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Decision Tree Regressor MSE: {mse:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(housing.feature_names, regressor.feature_importances_)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Decision Tree Regressor - Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewQaWcQf4Zns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the California housing dataset (built-in in sklearn).\n",
        "* Splits the dataset into training (80%) and testing (20%) sets.\n",
        "* Uses \"squared_error\" as the criterion (default in Scikit-Learn for regression).\n",
        "* Trains a Decision Tree Regressor with max_depth=5 to prevent overfitting.\n",
        "* Computes and prints the Mean Squared Error (MSE) to evaluate the model."
      ],
      "metadata": {
        "id": "VJsLhivn4cvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.\n",
        "**Ans** - Python program to train a Decision Tree Classifier and visualize the tree using Graphviz.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "y8cark1AfpnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import graphviz\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True, special_characters=True)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "XIRukX8U545h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits the data into 80% training and 20% testing.\n",
        "* Trains a Decision Tree Classifier using Gini Impurity.\n",
        "* Computes and prints model accuracy.\n",
        "* Generates a visual representation of the tree using Graphviz & Pydotplus."
      ],
      "metadata": {
        "id": "8sXCodRq59hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n",
        "**Ans** - Python program that trains a Decision Tree Classifier with a maximum depth of 3 and compares its accuracy with a fully grown tree (no depth restriction).\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "Ypo0UnzCfp34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_limited = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "clf_full = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Decision Tree Accuracy (Max Depth = 3): {accuracy_limited:.2f}\")\n",
        "print(f\"Decision Tree Accuracy (Fully Grown): {accuracy_full:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plot_tree(clf_limited, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.title(\"Decision Tree (Max Depth = 3)\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plot_tree(clf_full, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.title(\"Fully Grown Decision Tree\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MieyA1Hp6VnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains two Decision Tree models:\n",
        "  * One with max_depth=3 (to prevent overfitting).\n",
        "  * One fully grown (no depth restriction).\n",
        "* Computes and prints accuracy for both models."
      ],
      "metadata": {
        "id": "hmhKAvvs6Zpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree.\n",
        "**Ans** - Python program that trains a Decision Tree Classifier using min_samples_split=5 and compares its accuracy with a default decision tree.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "G_HiWWdUfqJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_limited = DecisionTreeClassifier(criterion=\"gini\", min_samples_split=5, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "clf_default = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "print(f\"Decision Tree Accuracy (min_samples_split = 5): {accuracy_limited:.2f}\")\n",
        "print(f\"Decision Tree Accuracy (Default Settings): {accuracy_default:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plot_tree(clf_limited, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.title(\"Decision Tree (min_samples_split = 5)\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plot_tree(clf_default, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.title(\"Default Decision Tree\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BPmjpVuU7BO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains two Decision Tree models:\n",
        "  * One with min_samples_split=5 (requires at least 5 samples to split a node).\n",
        "  * One with default parameters (typically min_samples_split=2).\n",
        "* Computes and prints accuracy for both models."
      ],
      "metadata": {
        "id": "GWzKiKrO7E2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data.\n",
        "**Ans** - Python program that applies feature scaling before training a Decision Tree Classifier and compares its accuracy with unscaled data.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "eAfw_U6afqdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_unscaled = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Decision Tree Accuracy (Unscaled Data): {accuracy_unscaled:.2f}\")\n",
        "print(f\"Decision Tree Accuracy (Scaled Data): {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "TVy8-hdI7qqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains two Decision Tree models:\n",
        "  * One on raw (unscaled) data.\n",
        "  * One after Standard Scaling (mean=0, variance=1).\n",
        "* Computes and prints accuracy for both models."
      ],
      "metadata": {
        "id": "-rwKrDYh7uMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.\n",
        "**Ans** -Python program to train a Decision Tree Classifier using the One-vs-Rest (OvR) strategy for multiclass classification.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "BpHON7jrfquM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier(criterion=\"gini\", random_state=42))\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy (OvR): {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "pKkNLSgp8GQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset (which has 3 classes).\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Uses One-vs-Rest (OvR) strategy, where:\n",
        "  * Each class is treated as one-vs-all (binary classification).\n",
        "  * A separate Decision Tree model is trained for each class.\n",
        "* Computes and prints model accuracy."
      ],
      "metadata": {
        "id": "8oJylLCE8J6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores.\n",
        "**Ans** - Python program to train a Decision Tree Classifier and display the feature importance scores.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "1Nxn0cGKfrBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(iris.feature_names, feature_importances, color='skyblue')\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance in Decision Tree\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OBS7caMX2M-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains a Decision Tree Classifier using the Gini criterion.\n",
        "* Computes and prints model accuracy.\n",
        "* Extracts and prints feature importance scores.\n",
        "* Visualizes feature importance using a bar chart.\n",
        "\n",
        "**Example Output (Feature Importances May Vary)**"
      ],
      "metadata": {
        "id": "CIpH2h5o2Q95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Decision Tree Model Accuracy: 1.00\n",
        "sepal length (cm): 0.02\n",
        "sepal width (cm): 0.00\n",
        "petal length (cm): 0.57\n",
        "petal width (cm): 0.41"
      ],
      "metadata": {
        "id": "D1UusgrM2caC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree.\n",
        "**Ans** - Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree using Mean Squared Error (MSE).\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "NzwQISsefrR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "\n",
        "reg_full = DecisionTreeRegressor(random_state=42)\n",
        "reg_full.fit(X_train, y_train)\n",
        "\n",
        "y_pred_limited = reg_limited.predict(X_test)\n",
        "y_pred_full = reg_full.predict(X_test)\n",
        "\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Decision Tree Regressor MSE (Max Depth = 5): {mse_limited:.4f}\")\n",
        "print(f\"Decision Tree Regressor MSE (Fully Grown): {mse_full:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.scatter(y_test, y_pred_limited, color='blue', alpha=0.5, label='Max Depth = 5')\n",
        "plt.scatter(y_test, y_pred_full, color='red', alpha=0.3, label='Fully Grown Tree')\n",
        "plt.plot([0, 5], [0, 5], color='black', linestyle='--', linewidth=2)\n",
        "\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Decision Tree Regression: Predicted vs Actual\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yRXc7UBv2sDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the California Housing dataset (predicts median house value).\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains two Decision Tree Regressors:\n",
        "  * One with max_depth=5 (to reduce overfitting).\n",
        "  * One fully grown (no depth restriction).\n",
        "* Computes and prints Mean Squared Error (MSE) for both models.\n",
        "* Plots actual vs predicted values for better comparison."
      ],
      "metadata": {
        "id": "5mO2gjYe2vmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n",
        "**Ans** - Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "GcMfO57pfrj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "ccp_path = clf_full.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = ccp_path.ccp_alphas[:-1]\n",
        "\n",
        "accuracy_train = []\n",
        "accuracy_test = []\n",
        "\n",
        "for alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    acc_train = accuracy_score(y_train, clf.predict(X_train))\n",
        "    acc_test = accuracy_score(y_test, clf.predict(X_test))\n",
        "\n",
        "    accuracy_train.append(acc_train)\n",
        "    accuracy_test.append(acc_test)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, accuracy_train, marker=\"o\", label=\"Training Accuracy\", color=\"blue\")\n",
        "plt.plot(ccp_alphas, accuracy_test, marker=\"o\", label=\"Testing Accuracy\", color=\"red\")\n",
        "plt.xlabel(\"CCP Alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IveOGZNd86s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains a fully grown Decision Tree to extract CCP alphas.\n",
        "* Iterates over different pruning strengths (ccp_alpha):\n",
        "  * Trains multiple trees.\n",
        "  * Records training and testing accuracy.\n",
        "* Plots accuracy vs. CCP alpha to visualize the impact of pruning.\n",
        "\n",
        "**Expected Output (Graph Analysis):**\n",
        "* Low ccp_alpha (left side) → Overfitting, high training accuracy but low testing accuracy.\n",
        "* Optimal ccp_alpha (middle) → Balanced, good generalization.\n",
        "* High ccp_alpha (right side) → Underfitting, both accuracies drop."
      ],
      "metadata": {
        "id": "sYHskriT8-0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "**Ans** - Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "6a-EFPxrfr2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "w4AX-lfc9cEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset (multiclass classification).\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains a Decision Tree Classifier using the Gini impurity criterion.\n",
        "* Computes key performance metrics:\n",
        "  * Precision → Measures class-specific correctness.\n",
        "  * Recall → Measures class-specific completeness.\n",
        "  * F1-Score → Balances precision and recall.\n",
        "* Uses average='weighted' since the Iris dataset is multiclass.\n",
        "\n",
        "**Example Output (Values May Vary):**"
      ],
      "metadata": {
        "id": "biVvlHNY9gDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy: 1.00\n",
        "Precision: 1.00\n",
        "Recall: 1.00\n",
        "F1-Score: 1.00"
      ],
      "metadata": {
        "id": "pMXxvame9szj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.\n",
        "**Ans** - Python program to train a Decision Tree Classifier and visualize the Confusion Matrix using seaborn.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "Oh8D9swtfsKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix - Decision Tree Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "upkzjXNH96Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset (3-class classification).\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Trains a Decision Tree Classifier using Gini impurity.\n",
        "* Computes Confusion Matrix to analyze misclassifications.\n",
        "* Visualizes the Confusion Matrix using seaborn.heatmap().\n",
        "\n",
        "**Example Output (Confusion Matrix):**"
      ],
      "metadata": {
        "id": "ljOacnCO9-Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Decision Tree Classifier Accuracy: 1.00"
      ],
      "metadata": {
        "id": "7dFOdsva-PDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Confusion Matrix Example:"
      ],
      "metadata": {
        "id": "OWTg9jYo-RS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[10  0  0]\n",
        " [ 0  9  1]\n",
        " [ 0  0 10]]"
      ],
      "metadata": {
        "id": "E2SWaNUf-VOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n",
        "**Ans** - Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n",
        "\n",
        "**Python Code**"
      ],
      "metadata": {
        "id": "2naBcat5fsd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Decision Tree Classifier Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "MgRLMCSY-twR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Loads the Iris dataset.\n",
        "* Splits data into 80% training and 20% testing.\n",
        "* Defines a Decision Tree Classifier.\n",
        "* Uses GridSearchCV to optimize:\n",
        "  * max_depth (limits tree depth).\n",
        "  * min_samples_split (controls node splitting).\n",
        "* Uses 5-fold cross-validation (cv=5) for better tuning.\n",
        "* Trains the best model and evaluates it on the test set.\n",
        "\n",
        "**Example Output**"
      ],
      "metadata": {
        "id": "h03Xl0Ae-xa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
        "Best Decision Tree Classifier Accuracy: 1.00"
      ],
      "metadata": {
        "id": "PKWXdWqj_AZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}